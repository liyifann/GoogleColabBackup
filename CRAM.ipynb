{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CRAM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "mount_file_id": "1S3559E22AfpRzFrvUOqeFtPsSRtehBQ4",
      "authorship_tag": "ABX9TyPZeWJHEMVmEHye+ddeWPgt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/liyifann/GoogleColabBackup/blob/master/CRAM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CJrFfueZbonl",
        "colab_type": "code",
        "outputId": "cfa4744b-7eb2-4472-d717-ab0f039b3e30",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 703
        }
      },
      "source": [
        "pip install tensorflow=='1.14.0'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/f0/96fb2e0412ae9692dbf400e5b04432885f677ad6241c088ccc5fe7724d69/tensorflow-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (109.2MB)\n",
            "\u001b[K     |████████████████████████████████| 109.2MB 92kB/s \n",
            "\u001b[?25hCollecting tensorboard<1.15.0,>=1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/91/2d/2ed263449a078cd9c8a9ba50ebd50123adf1f8cfbea1492f9084169b89d9/tensorboard-1.14.0-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.2MB 50.6MB/s \n",
            "\u001b[?25hRequirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (3.10.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.18.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Collecting tensorflow-estimator<1.15.0rc0,>=1.14.0rc0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/3c/d5/21860a5b11caf0678fbc8319341b0ae21a07156911132e0e71bffed0510d/tensorflow_estimator-1.14.0-py2.py3-none-any.whl (488kB)\n",
            "\u001b[K     |████████████████████████████████| 491kB 53.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.28.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow==1.14.0) (1.12.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (3.2.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow==1.14.0) (46.1.3)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow==1.14.0) (2.10.0)\n",
            "Installing collected packages: tensorboard, tensorflow-estimator, tensorflow\n",
            "  Found existing installation: tensorboard 2.2.0\n",
            "    Uninstalling tensorboard-2.2.0:\n",
            "      Successfully uninstalled tensorboard-2.2.0\n",
            "  Found existing installation: tensorflow-estimator 2.2.0rc0\n",
            "    Uninstalling tensorflow-estimator-2.2.0rc0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.2.0rc0\n",
            "  Found existing installation: tensorflow 2.2.0rc3\n",
            "    Uninstalling tensorflow-2.2.0rc3:\n",
            "      Successfully uninstalled tensorflow-2.2.0rc3\n",
            "Successfully installed tensorboard-1.14.0 tensorflow-1.14.0 tensorflow-estimator-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a4HKQk5qb33N",
        "colab_type": "code",
        "outputId": "5362a5dd-8bd4-4f2c-e322-79a684010ede",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        }
      },
      "source": [
        "pip install tensorflow-gpu=='1.14.0'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==1.14.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/76/04/43153bfdfcf6c9a4c38ecdb971ca9a75b9a791bb69a764d652c359aca504/tensorflow_gpu-1.14.0-cp36-cp36m-manylinux1_x86_64.whl (377.0MB)\n",
            "\u001b[K     |████████████████████████████████| 377.0MB 44kB/s \n",
            "\u001b[?25hRequirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.34.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.2.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.8.1)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.3.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.18.2)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (3.10.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.28.1)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.14.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (0.9.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==1.14.0) (1.12.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==1.14.0) (2.10.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (3.2.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow-gpu==1.14.0) (46.1.3)\n",
            "Installing collected packages: tensorflow-gpu\n",
            "Successfully installed tensorflow-gpu-1.14.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6g3ng12LGD5",
        "colab_type": "code",
        "outputId": "162e4208-676f-40ec-bc1f-39c9728a46fa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 534
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "tf.__version__"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
            "/usr/local/lib/python3.6/dist-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
            "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<p style=\"color: red;\">\n",
              "The default version of TensorFlow in Colab will switch to TensorFlow 2.x on the 27th of March, 2020.<br>\n",
              "We recommend you <a href=\"https://www.tensorflow.org/guide/migrate\" target=\"_blank\">upgrade</a> now\n",
              "or ensure your notebook will continue to use TensorFlow 1.x via the <code>%tensorflow_version 1.x</code> magic:\n",
              "<a href=\"https://colab.research.google.com/notebooks/tensorflow_version.ipynb\" target=\"_blank\">more info</a>.</p>\n"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'1.14.0'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tlt2oFSVI3Fy",
        "colab_type": "code",
        "outputId": "55492bf6-3960-435e-b2e3-1363c3e38af6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0xzyO0nFl_l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#! /usr/bin/python3\n",
        "# import tensorflow as tf\n",
        "\n",
        "class cnn:\n",
        "\tdef __init__(\n",
        "\t\t\tself,\n",
        "\t\t\tweight_stddev\t= 0.1,\n",
        "\t\t\tbias_constant\t= 0.1,\n",
        "\t\t\tpadding\t\t\t= \"SAME\",\n",
        "\t\t\t):\n",
        "\t\t\tself.weight_stddev\t= weight_stddev\n",
        "\t\t\tself.bias_constant\t= bias_constant\n",
        "\t\t\tself.padding\t\t= padding\n",
        "\n",
        "\tdef weight_variable(self, shape):\n",
        "\t\tinitial = tf.truncated_normal(shape, stddev = self.weight_stddev)\n",
        "\t\treturn tf.Variable(initial)\n",
        "\n",
        "\n",
        "\tdef bias_variable(self, shape):\n",
        "\t\tinitial = tf.constant(self.bias_constant, shape = shape)\n",
        "\t\treturn tf.Variable(initial)\n",
        "\n",
        "\n",
        "\tdef conv1d(self, x, W, kernel_stride):\n",
        "\t# API: must strides[0]=strides[4]=1\n",
        "\t\treturn tf.nn.conv1d(x, W, stride=kernel_stride, padding=self.padding)\n",
        "\n",
        "\n",
        "\tdef conv2d(self, x, W, kernel_stride):\n",
        "\t# API: must strides[0]=strides[4]=1\n",
        "\t\treturn tf.nn.conv2d(x, W, strides=[1, kernel_stride, kernel_stride, 1], padding=self.padding)\n",
        "\n",
        "\n",
        "\tdef conv3d(self, x, W, kernel_stride):\n",
        "\t# API: must strides[0]=strides[4]=1\n",
        "\t\treturn tf.nn.conv3d(x, W, strides=[1, kernel_stride, kernel_stride, kernel_stride, 1], padding=self.padding)\n",
        "\n",
        "\n",
        "\tdef apply_conv1d(self, x, filter_width, in_channels, out_channels, kernel_stride, train_phase):\n",
        "\t\tweight = self.weight_variable([filter_width, in_channels, out_channels])\n",
        "\t\tbias = self.bias_variable([out_channels]) # each feature map shares the same weight and bias\n",
        "\t\tconv_1d = tf.add(self.conv1d(x, weight, kernel_stride), bias)\n",
        "\t\tconv_1d_bn = self.batch_norm_cnv_1d(conv_1d, train_phase)\n",
        "\t\treturn tf.nn.relu(conv_1d_bn)\n",
        "\n",
        "\n",
        "\tdef apply_conv2d(self, x, filter_height, filter_width, in_channels, out_channels, kernel_stride, train_phase):\n",
        "\t\tweight = self.weight_variable([filter_height, filter_width, in_channels, out_channels])\n",
        "\t\tbias = self.bias_variable([out_channels]) # each feature map shares the same weight and bias\n",
        "\t\tconv_2d = tf.add(self.conv2d(x, weight, kernel_stride), bias)\n",
        "\t\tconv_2d_bn = self.batch_norm_cnv_2d(conv_2d, train_phase)\n",
        "\t\treturn tf.nn.relu(conv_2d_bn)\n",
        "\n",
        "\n",
        "\t\n",
        "\tdef apply_conv3d(self, x, filter_depth, filter_height, filter_width, in_channels, out_channels, kernel_stride, train_phase):\n",
        "\t\tweight = self.weight_variable([filter_depth, filter_height, filter_width, in_channels, out_channels])\n",
        "\t\tbias = self.bias_variable([out_channels]) # each feature map shares the same weight and bias\n",
        "\t\tconv_3d = tf.add(self.conv3d(x, weight, kernel_stride), bias)\n",
        "\t\tconv_3d_bn = self.batch_norm_cnv_3d(conv_3d, train_phase)\n",
        "\t\treturn tf.nn.relu(conv_3d_bn)\n",
        "\n",
        "\n",
        "\tdef batch_norm_cnv_3d(self, inputs, train_phase):\n",
        "\t\treturn tf.layers.batch_normalization(inputs, axis=4, momentum=0.993, epsilon=1e-5, scale=False, training=train_phase)\n",
        "\n",
        "\n",
        "\tdef batch_norm_cnv_2d(self, inputs, train_phase):\n",
        "\t\treturn tf.layers.batch_normalization(inputs, axis=3, momentum=0.993, epsilon=1e-5, scale=False, training=train_phase)\n",
        "\n",
        "\n",
        "\tdef batch_norm_cnv_1d(self, inputs, train_phase):\n",
        "\t\treturn tf.layers.batch_normalization(inputs, axis=2, momentum=0.993, epsilon=1e-5, scale=False, training=train_phase)\n",
        "\n",
        "\n",
        "\tdef batch_norm(self, inputs, train_phase):\n",
        "\t\treturn tf.layers.batch_normalization(inputs, axis=1, momentum=0.993, epsilon=1e-5, scale=False, training=train_phase)\n",
        "\n",
        "\n",
        "\tdef apply_max_pooling(self, x, pooling_height, pooling_width, pooling_stride):\n",
        "\t# API: must ksize[0]=ksize[4]=1, strides[0]=strides[4]=1\n",
        "\t\treturn tf.nn.max_pool(x, ksize=[1, pooling_height, pooling_width, 1], strides=[1, pooling_stride, pooling_stride, 1], padding=self.padding)\n",
        "\n",
        "\n",
        "\tdef apply_max_pooling3d(self, x, pooling_depth, pooling_height, pooling_width, pooling_stride):\n",
        "\t# API: must ksize[0]=ksize[4]=1, strides[0]=strides[4]=1\n",
        "\t\treturn tf.nn.max_pool3d(x, ksize=[1, pooling_depth, pooling_height, pooling_width, 1], strides=[1, pooling_stride, pooling_stride, pooling_stride, 1], padding=self.padding)\n",
        "\n",
        "\t\n",
        "\tdef apply_fully_connect(self, x, x_size, fc_size, train_phase):\n",
        "\t\tfc_weight = self.weight_variable([x_size, fc_size])\n",
        "\t\tfc_bias = self.bias_variable([fc_size])\n",
        "\t\tfc = tf.add(tf.matmul(x, fc_weight), fc_bias)\n",
        "\t\tfc_bn = self.batch_norm(fc, train_phase)\n",
        "\t\treturn tf.nn.relu(fc_bn)\n",
        "\n",
        "\t\n",
        "\tdef apply_readout(self, x, x_size, readout_size):\n",
        "\t\treadout_weight = self.weight_variable([x_size, readout_size])\n",
        "\t\treadout_bias = self.bias_variable([readout_size])\n",
        "\t\treturn tf.add(tf.matmul(x, readout_weight), readout_bias)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gSWgs0WwF9xi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# import tensorflow as tf\n",
        "\n",
        "\n",
        "def attention(inputs, attention_size, time_major=False, return_alphas=False, train_phase=True):\n",
        "    \"\"\"\n",
        "    Attention mechanism layer which reduces RNN/Bi-RNN outputs with Attention vector.\n",
        "    \n",
        "    Args:\n",
        "        inputs: The Attention inputs.\n",
        "            Matches outputs of RNN/Bi-RNN layer (not final state):\n",
        "                In case of RNN, this must be RNN outputs `Tensor`:\n",
        "                    If time_major == False (default), this must be a tensor of shape:\n",
        "                        `[batch_size, max_time, cell.output_size]`.\n",
        "                    If time_major == True, this must be a tensor of shape:\n",
        "                        `[max_time, batch_size, cell.output_size]`.\n",
        "                In case of Bidirectional RNN, this must be a tuple (outputs_fw, outputs_bw) containing the forward and\n",
        "                the backward RNN outputs `Tensor`.\n",
        "                    If time_major == False (default),\n",
        "                        outputs_fw is a `Tensor` shaped:\n",
        "                        `[batch_size, max_time, cell_fw.output_size]`\n",
        "                        and outputs_bw is a `Tensor` shaped:\n",
        "                        `[batch_size, max_time, cell_bw.output_size]`.\n",
        "                    If time_major == True,\n",
        "                        outputs_fw is a `Tensor` shaped:\n",
        "                        `[max_time, batch_size, cell_fw.output_size]`\n",
        "                        and outputs_bw is a `Tensor` shaped:\n",
        "                        `[max_time, batch_size, cell_bw.output_size]`.\n",
        "        attention_size: Linear size of the Attention weights.\n",
        "        time_major: The shape format of the `inputs` Tensors.\n",
        "            If true, these `Tensors` must be shaped `[max_time, batch_size, depth]`.\n",
        "            If false, these `Tensors` must be shaped `[batch_size, max_time, depth]`.\n",
        "            Using `time_major = True` is a bit more efficient because it avoids\n",
        "            transposes at the beginning and end of the RNN calculation.  However,\n",
        "            most TensorFlow data is batch-major, so by default this function\n",
        "            accepts input and emits output in batch-major form.\n",
        "        return_alphas: Whether to return attention coefficients variable along with layer's output.\n",
        "            Used for visualization purpose.\n",
        "    Returns:\n",
        "        The Attention output `Tensor`.\n",
        "        In case of RNN, this will be a `Tensor` shaped:\n",
        "            `[batch_size, cell.output_size]`.\n",
        "        In case of Bidirectional RNN, this will be a `Tensor` shaped:\n",
        "            `[batch_size, cell_fw.output_size + cell_bw.output_size]`.\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(inputs, tuple):\n",
        "        # In case of Bi-RNN, concatenate the forward and the backward RNN outputs.\n",
        "        inputs = tf.concat(inputs, 2)\n",
        "\n",
        "    if time_major:\n",
        "        # (T,B,D) => (B,T,D)\n",
        "        inputs = tf.array_ops.transpose(inputs, [1, 0, 2])\n",
        "\n",
        "    hidden_size = inputs.shape[2].value  # D value - hidden size of the RNN layer\n",
        "\n",
        "    # Trainable parameters\n",
        "    w_omega = tf.Variable(tf.random_normal([hidden_size, attention_size], stddev=0.1))\n",
        "    b_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
        "    u_omega = tf.Variable(tf.random_normal([attention_size], stddev=0.1))\n",
        "\n",
        "    with tf.name_scope('v'):\n",
        "        # Applying fully connected layer with non-linear activation to each of the B*T timestamps;\n",
        "        #  the shape of `v` is (B,T,D)*(D,A)=(B,T,A), where A=attention_size\n",
        "        v = tf.tanh(tf.tensordot(inputs, w_omega, axes=1) + b_omega)\n",
        "    # For each of the timestamps its vector of size A from `v` is reduced with `u` vector\n",
        "    vu = tf.tensordot(v, u_omega, axes=1, name='vu')  # (B,T) shape\n",
        "    alphas = tf.nn.softmax(vu, name='alphas')         # (B,T) shape\n",
        "\n",
        "    # Output of (Bi-)RNN is reduced with attention vector; the result has (B,D) shape\n",
        "    output = tf.reduce_sum(inputs * tf.expand_dims(alphas, -1), 1)\n",
        "\n",
        "    if not return_alphas:\n",
        "        return output\n",
        "    else:\n",
        "        return output, alphas\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zjUG3Gypnvm8",
        "colab_type": "code",
        "outputId": "1923b1c5-c6b2-4a6a-8d75-7fc02bca59fb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#! /usr/bin/python3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "#from cnn_class import cnn\n",
        "import time\n",
        "import scipy.io as sio\n",
        "from sklearn.metrics import classification_report, roc_auc_score, auc, roc_curve, f1_score, precision_score, \\\n",
        "    recall_score\n",
        "#from RnnAttention.attention import attention\n",
        "from scipy import interp\n",
        "\n",
        "\n",
        "def multiclass_roc_auc_score(y_true, y_score):\n",
        "    assert y_true.shape == y_score.shape\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    n_classes = y_true.shape[1]\n",
        "    # compute ROC curve and ROC area for each class\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    # compute micro-average ROC curve and ROC area\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_score.ravel())\n",
        "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    # compute macro-average ROC curve and ROC area\n",
        "    # First aggregate all false probtive rates\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "    # Then interpolate all ROC curves at this points\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes):\n",
        "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "    # Finally average it and compute AUC\n",
        "    mean_tpr /= n_classes\n",
        "    fpr[\"macro\"] = all_fpr\n",
        "    tpr[\"macro\"] = mean_tpr\n",
        "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "    return roc_auc\n",
        "\n",
        "###########################################################################\n",
        "# prepare raw data\n",
        "###########################################################################\n",
        "subject_id = 1\n",
        "data_folder = '/content/drive/My Drive/Colab Notebooks/'\n",
        "data = sio.loadmat(data_folder+\"/5_cross_sub/cross_subject_data_\"+str(subject_id)+\".mat\")\n",
        "print(\"subject id \", subject_id)\n",
        "\n",
        "test_X\t= data[\"test_x\"] # [trials, channels, time length]\n",
        "train_X\t= data[\"train_x\"]\n",
        "\n",
        "test_y\t= data[\"test_y\"].ravel()\n",
        "train_y = data[\"train_y\"].ravel()\n",
        "\n",
        "\n",
        "train_y = np.asarray(pd.get_dummies(train_y), dtype = np.int8)\n",
        "test_y = np.asarray(pd.get_dummies(test_y), dtype = np.int8)\n",
        "\n",
        "###########################################################################\n",
        "# crop data\n",
        "###########################################################################\n",
        "\n",
        "window_size = 400\n",
        "step = 50\n",
        "n_channel = 22\n",
        "\n",
        "\n",
        "def windows(data, size, step):\n",
        "\tstart = 0\n",
        "\twhile ((start+size) < data.shape[0]):\n",
        "\t\tyield int(start), int(start + size)\n",
        "\t\tstart += step\n",
        "\n",
        "\n",
        "def segment_signal_without_transition(data, window_size, step):\n",
        "\tsegments = []\n",
        "\tfor (start, end) in windows(data, window_size, step):\n",
        "\t\tif(len(data[start:end]) == window_size):\n",
        "\t\t\tsegments = segments + [data[start:end]]\n",
        "\treturn np.array(segments)\n",
        "\n",
        "\n",
        "def segment_dataset(X, window_size, step):\n",
        "\twin_x = []\n",
        "\tfor i in range(X.shape[0]):\n",
        "\t\twin_x = win_x + [segment_signal_without_transition(X[i], window_size, step)]\n",
        "\twin_x = np.array(win_x)\n",
        "\treturn win_x\n",
        "\n",
        "\n",
        "train_raw_x = np.transpose(train_X, [0, 2, 1])\n",
        "test_raw_x = np.transpose(test_X, [0, 2, 1])\n",
        "\n",
        "\n",
        "train_win_x = segment_dataset(train_raw_x, window_size, step)\n",
        "print(\"train_win_x shape: \", train_win_x.shape)\n",
        "test_win_x = segment_dataset(test_raw_x, window_size, step)\n",
        "print(\"test_win_x shape: \", test_win_x.shape)\n",
        "\n",
        "# [trial, window, channel, time_length]\n",
        "train_win_x = np.transpose(train_win_x, [0, 1, 3, 2])\n",
        "print(\"train_win_x shape: \", train_win_x.shape)\n",
        "\n",
        "test_win_x = np.transpose(test_win_x, [0, 1, 3, 2])\n",
        "print(\"test_win_x shape: \", test_win_x.shape)\n",
        "\n",
        "\n",
        "# [trial, window, channel, time_length, 1]\n",
        "train_x = np.expand_dims(train_win_x, axis = 4)\n",
        "test_x = np.expand_dims(test_win_x, axis = 4)\n",
        "\n",
        "num_timestep = train_x.shape[1]\n",
        "###########################################################################\n",
        "# set model parameters\n",
        "###########################################################################\n",
        "# kernel parameter\n",
        "kernel_height_1st\t= 22\n",
        "kernel_width_1st \t= 45\n",
        "\n",
        "kernel_stride\t\t= 1\n",
        "\n",
        "conv_channel_num\t= 40\n",
        "\n",
        "# pooling parameter\n",
        "pooling_height_1st \t= 1\n",
        "pooling_width_1st \t= 75\n",
        "\n",
        "pooling_stride_1st = 10\n",
        "\n",
        "# full connected parameter\n",
        "attention_size = 512\n",
        "n_hidden_state = 64\n",
        "\n",
        "###########################################################################\n",
        "# set dataset parameters\n",
        "###########################################################################\n",
        "# input channel\n",
        "input_channel_num = 1\n",
        "\n",
        "# input height \n",
        "input_height = train_x.shape[2]\n",
        "\n",
        "# input width\n",
        "input_width = train_x.shape[3]\n",
        "\n",
        "# prediction class\n",
        "num_labels = 4\n",
        "###########################################################################\n",
        "# set training parameters\n",
        "###########################################################################\n",
        "# set learning rate\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# set maximum traing epochs\n",
        "training_epochs = 3\n",
        "\n",
        "# set batch size\n",
        "batch_size = 10\n",
        "\n",
        "# set dropout probability\n",
        "dropout_prob = 0.5\n",
        "\n",
        "# set train batch number per epoch\n",
        "batch_num_per_epoch = train_x.shape[0]//batch_size\n",
        "\n",
        "# instance cnn class\n",
        "padding = 'VALID'\n",
        "\n",
        "cnn_2d = cnn(padding=padding)\n",
        "\n",
        "# input placeholder\n",
        "X = tf.placeholder(tf.float32, shape=[None, input_height, input_width, input_channel_num], name = 'X')\n",
        "Y = tf.placeholder(tf.float32, shape=[None, num_labels], name = 'Y')\n",
        "train_phase = tf.placeholder(tf.bool, name = 'train_phase')\n",
        "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "\n",
        "# first CNN layer\n",
        "conv_1 = cnn_2d.apply_conv2d(X, kernel_height_1st, kernel_width_1st, input_channel_num, conv_channel_num, kernel_stride, train_phase)\n",
        "print(\"conv 1 shape: \", conv_1.get_shape().as_list())\n",
        "pool_1 = cnn_2d.apply_max_pooling(conv_1, pooling_height_1st, pooling_width_1st, pooling_stride_1st)\n",
        "print(\"pool 1 shape: \", pool_1.get_shape().as_list())\n",
        "\n",
        "pool1_shape = pool_1.get_shape().as_list()\n",
        "pool1_flat = tf.reshape(pool_1, [-1, pool1_shape[1]*pool1_shape[2]*pool1_shape[3]])\n",
        "\n",
        "fc_drop = tf.nn.dropout(pool1_flat, keep_prob)\t\n",
        "\n",
        "lstm_in = tf.reshape(fc_drop, [-1, num_timestep, pool1_shape[1]*pool1_shape[2]*pool1_shape[3]])\n",
        "\n",
        "########################## RNN ########################\n",
        "cells = []\n",
        "for _ in range(2):\n",
        "\tcell = tf.contrib.rnn.BasicLSTMCell(n_hidden_state, forget_bias=1.0, state_is_tuple=True)\n",
        "\tcell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
        "\tcells.append(cell)\n",
        "lstm_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
        "\n",
        "init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
        "\n",
        "# output ==> [batch, step, n_hidden_state]\n",
        "rnn_op, states = tf.nn.dynamic_rnn(lstm_cell, lstm_in, initial_state=init_state, time_major=False)\n",
        "\n",
        "########################## attention ########################\n",
        "with tf.name_scope('Attention_layer'):\n",
        "    attention_op, alphas = attention(rnn_op, attention_size, time_major = False, return_alphas=True)\n",
        "\n",
        "attention_drop = tf.nn.dropout(attention_op, keep_prob)\t\n",
        "\n",
        "########################## readout ########################\n",
        "y_ = cnn_2d.apply_readout(attention_drop, rnn_op.shape[2].value, num_labels)\n",
        "\n",
        "# probability prediction \n",
        "y_prob = tf.nn.softmax(y_, name = \"y_prob\")\n",
        "\n",
        "# class prediction \n",
        "y_pred = tf.argmax(y_prob, 1, name = \"y_pred\")\n",
        "\n",
        "########################## loss and optimizer ########################\n",
        "# cross entropy cost function\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=Y), name = 'loss')\n",
        "\n",
        "\n",
        "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "with tf.control_dependencies(update_ops):\n",
        "\t# set training SGD optimizer\n",
        "\toptimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# get correctly predicted object\n",
        "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(y_), 1), tf.argmax(Y, 1))\n",
        "\n",
        "########################## define accuracy ########################\n",
        "# calculate prediction accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name = 'accuracy')\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "# train test and save result\n",
        "###########################################################################\n",
        "\n",
        "# run with gpu memory growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "best_test_acc = []\n",
        "train_loss = []\n",
        "with tf.Session(config=config) as session:\n",
        "\tsession.run(tf.global_variables_initializer())\n",
        "\tbest_acc = 0\n",
        "\tfor epoch in range(training_epochs):\n",
        "\t\tpred_test = np.array([])\n",
        "\t\ttrue_test = []\n",
        "\t\tprob_test = []\n",
        "\t\tp=[]\n",
        "\t\tr=[]\n",
        "\t\tf=[]\n",
        "\t\tauc_roc_test=[]\n",
        "\t\t########################## training process ########################\n",
        "\t\tfor b in range(batch_num_per_epoch):\n",
        "\t\t\toffset = (b * batch_size) % (train_y.shape[0] - batch_size) \n",
        "\t\t\tbatch_x = train_x[offset:(offset + batch_size), :, :, :, :]\n",
        "\t\t\tbatch_x = batch_x.reshape([len(batch_x)*num_timestep, n_channel, window_size, 1])\n",
        "\t\t\tbatch_y = train_y[offset:(offset + batch_size), :]\n",
        "\t\t\t_, c = session.run([optimizer, cost], feed_dict={X: batch_x, Y: batch_y, keep_prob: 1-dropout_prob, train_phase: True})\n",
        "\t\t# calculate train and test accuracy after each training epoch\n",
        "\t\tif(epoch%1 == 0):\n",
        "\t\t\ttrain_accuracy \t= np.zeros(shape=[0], dtype=float)\n",
        "\t\t\ttest_accuracy\t= np.zeros(shape=[0], dtype=float)\n",
        "\t\t\ttrain_l \t\t= np.zeros(shape=[0], dtype=float)\n",
        "\t\t\ttest_l\t\t\t= np.zeros(shape=[0], dtype=float)\n",
        "\t\t\t# calculate train accuracy after each training epoch\n",
        "\t\t\tfor i in range(batch_num_per_epoch):\n",
        "\t\t\t\t########################## prepare training data ########################\n",
        "\t\t\t\toffset = (i * batch_size) % (train_y.shape[0] - batch_size) \n",
        "\t\t\t\ttrain_batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
        "\t\t\t\ttrain_batch_x = train_batch_x.reshape([len(train_batch_x)*num_timestep, n_channel, window_size, 1])\n",
        "\t\t\t\ttrain_batch_y = train_y[offset:(offset + batch_size), :]\n",
        "\n",
        "\t\t\t\t########################## calculate training results ########################\n",
        "\t\t\t\ttrain_a, train_c = session.run([accuracy, cost], feed_dict={X: train_batch_x, Y: train_batch_y, keep_prob: 1.0, train_phase: False})\n",
        "\t\t\t\t\n",
        "\t\t\t\ttrain_l = np.append(train_l, train_c)\n",
        "\t\t\t\ttrain_accuracy = np.append(train_accuracy, train_a)\n",
        "\t\t\tprint(\"(\"+time.asctime(time.localtime(time.time()))+\") Epoch: \", epoch+1, \" Training Cost: \", np.mean(train_l), \"Training Accuracy: \", np.mean(train_accuracy))\n",
        "\t\t\ttrain_acc = train_acc + [np.mean(train_accuracy)]\n",
        "\t\t\ttrain_loss = train_loss + [np.mean(train_l)]\n",
        "\t\t\t# calculate test accuracy after each training epoch\n",
        "\t\t\tfor j in range(batch_num_per_epoch):\n",
        "\t\t\t\t########################## prepare test data ########################\n",
        "\t\t\t\toffset = (j * batch_size) % (test_y.shape[0] - batch_size) \n",
        "\t\t\t\ttest_batch_x = test_x[offset:(offset + batch_size), :, :, :]\n",
        "\t\t\t\ttest_batch_x = test_batch_x.reshape([len(test_batch_x)*num_timestep, n_channel, window_size, 1])\n",
        "\t\t\t\ttest_batch_y = test_y[offset:(offset + batch_size), :]\n",
        "\t\t\t\t\n",
        "\t\t\t\t########################## calculate test results ########################\n",
        "\t\t\t\ttest_a, test_c, prob_v, pred_v = session.run([accuracy, cost, y_prob, y_pred], feed_dict={X: test_batch_x, Y: test_batch_y, keep_prob: 1.0, train_phase: False})\n",
        "\t\t\t\t\n",
        "\t\t\t\ttest_accuracy = np.append(test_accuracy, test_a)\n",
        "\t\t\t\ttest_l = np.append(test_l, test_c)\n",
        "\t\t\t\tpred_test = np.append(pred_test, pred_v)\n",
        "\t\t\t\ttrue_test.append(test_batch_y)\n",
        "\t\t\t\tprob_test.append(prob_v)\n",
        "\t\t\t\tauc_roc_test1 = multiclass_roc_auc_score(y_true=test_batch_y, y_score=prob_v)\n",
        "\t\t\t\tauc_roc_test.append(auc_roc_test1['macro'])\n",
        "\t\t\t\tp1 = precision_score(y_true=np.argmax(test_batch_y, axis=1), y_pred=pred_v, average='macro')\n",
        "\t\t\t\tp.append(p1)\n",
        "\t\t\t\tr1 = recall_score(y_true=np.argmax(test_batch_y, axis=1), y_pred=pred_v, average='macro')\n",
        "\t\t\t\tr.append(r1)\n",
        "\t\t\t\tf1 = f1_score(y_true=np.argmax(test_batch_y, axis=1), y_pred=pred_v, average='macro')\n",
        "\t\t\t\tf.append(f1)\n",
        "\t\t\tif np.mean(test_accuracy) > best_acc :\n",
        "\t\t\t\tbest_acc = np.mean(test_accuracy)\n",
        "\t\t\ttrue_test = np.array(true_test).reshape([-1, num_labels])\n",
        "\t\t\tprob_test = np.array(prob_test).reshape([-1, num_labels])\n",
        "\t\t#\tp1 = precision_score(y_true=np.argmax(true_test, axis=1), y_pred=pred_test, average='macro')\n",
        "\t\t#\tr1 = recall_score(y_true=np.argmax(true_test, axis=1), y_pred=pred_test, average='macro')\n",
        "\t\t#\tf1 = f1_score(y_true=np.argmax(true_test, axis=1), y_pred=pred_test, average='macro')\n",
        "\t\t\t##auc_roc_test1 = multiclass_roc_auc_score(y_true=true_test, y_score=prob_test)\n",
        "\t\t\t##auc_roc_test=np.append(auc_roc_test1['macro'])\n",
        "\t\t#\tauc_roc_test1 = multiclass_roc_auc_score(y_true=np.argmax(true_test, axis=1), y_score=prob_test)\n",
        "\t\t\t#auc_roc_test=np.append(auc_roc_test1[None])\n",
        "\t\t\t##p = precision_score(y_true=np.argmax(true_test, axis=1), y_pred=pred_test, average='macro')\n",
        "\t\t#\tp = np.append(p1)\n",
        "\t\t\t##r = recall_score(y_true=np.argmax(true_test, axis=1), y_pred=pred_test, average='macro')\n",
        "\t\t#\tr = np.append(r1)\n",
        "\t\t\t##f = f1_score(y_true=np.argmax(true_test, axis=1), y_pred=pred_test, average='macro')\n",
        "\t\t#\tf = np.append(f1)\n",
        "\n",
        "\t\tprint(\"(\"+time.asctime(time.localtime(time.time()))+\") Epoch: \", epoch+1, \"Test Cost: \", np.mean(test_l),\"\\n\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \"Test Accuracy: \", np.mean(test_accuracy),\n",
        "\t\t\t\t  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \"Test Acc_Std:\",np.std(test_accuracy),\"\\n\",\n",
        "\t\t\t\t  \t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \"Test Precision:\",np.mean(p),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \"Test Pre_Std:\", np.std(p),\"\\n\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t      \"Test Recall_score:\", np.mean(r),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t          \"Test Rec_Std:\", np.std(r),\"\\n\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \"Test f1: \", np.mean(f),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \"Test f1_Std:\", np.std(f),\"\\n\",\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \"Test AUC: \", np.mean(auc_roc_test),\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \"auc_roc_test: \", auc_roc_test,\n",
        "                                            \"AUC:\", auc_roc_test1['macro'],\n",
        "\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t  \"Test AUC_Std:\",np.std(auc_roc_test),\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "subject id  1\n",
            "train_win_x shape:  (4032, 15, 400, 22)\n",
            "test_win_x shape:  (1152, 15, 400, 22)\n",
            "train_win_x shape:  (4032, 15, 22, 400)\n",
            "test_win_x shape:  (1152, 15, 22, 400)\n",
            "WARNING:tensorflow:From <ipython-input-3-5dd779426505>:68: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7fb30d1f9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7fb30d1f9e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7fb30d1f9e10>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7fb30d1f9e10>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "conv 1 shape:  [None, 1, 356, 40]\n",
            "pool 1 shape:  [None, 1, 29, 40]\n",
            "WARNING:tensorflow:From <ipython-input-5-2ae8772cbcc4>:185: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-5-2ae8772cbcc4>:192: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-5-2ae8772cbcc4>:195: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-5-2ae8772cbcc4>:200: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fb25b4d8518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fb25b4d8518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fb25b4d8518>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7fb25b4d8518>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fb25b4d80b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fb25b4d80b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fb25b4d80b8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fb25b4d80b8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fb2555e1be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fb2555e1be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fb2555e1be0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7fb2555e1be0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From <ipython-input-5-2ae8772cbcc4>:219: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "(Sat Apr 18 14:53:21 2020) Epoch:  1  Training Cost:  1.3856680600282276 Training Accuracy:  0.24987593579750794\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_ranking.py:808: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
            "  UndefinedMetricWarning)\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "(Sat Apr 18 14:53:30 2020) Epoch:  1 Test Cost:  1.386196578702619 \n",
            " Test Accuracy:  0.24962779678053063 Test Acc_Std: 0.12148220645982999 \n",
            " Test Precision: 0.06699981619336458 Test Pre_Std: 0.04251817652112449 \n",
            " Test Recall_score: 0.25516956162117455 Test Rec_Std: 0.043626199749119056 \n",
            " Test f1:  0.10112276405700725 Test f1_Std: 0.046299614117359146 \n",
            " Test AUC:  nan auc_roc_test:  [0.5033482142857143, 0.8220486111111112, 0.40480324074074076, 0.609375, 0.6904761904761905, 0.6257440476190476, 0.6901041666666667, 0.5958994708994709, 0.6364087301587302, 0.46484375, 0.82421875, 0.9231770833333334, 0.49617642195767186, 0.524375, nan, 0.40587797619047616, 0.5167410714285714, 0.5591517857142857, 0.5316220238095238, 0.7209821428571429, 0.5988888888888889, 0.7157118055555556, 0.6976273148148149, 0.673900462962963, 0.547951388888889, 0.6013144841269841, 0.7195767195767195, 0.6549479166666667, nan, 0.7548983134920635, 0.5272916666666667, 0.6875, 0.8518725198412699, 0.4980952380952381, 0.7122395833333334, 0.4836309523809524, 0.6304563492063492, 0.46322916666666664, 0.5533854166666666, 0.6045386904761905, 0.5912499999999999, 0.7252604166666667, 0.8143402777777777, 0.5345155423280423, 0.6555059523809523, 0.5230654761904762, 0.5228174603174602, 0.5849470899470899, 0.8668154761904763, 0.5316220238095238, 0.8198412698412698, 0.45281249999999995, 0.6953125, 0.7120535714285714, nan, 0.7447916666666666, 0.4716435185185185, 0.26269014550264547, 0.5646081349206349, 0.8544444444444445, 0.7039310515873016, 0.2869047619047619, 0.6518055555555555, 0.6361607142857142, 0.8794642857142858, 0.30687830687830686, 0.8154761904761904, 0.5037830687830689, 0.8783482142857143, 0.6860119047619049, 0.7250744047619048, 0.5264756944444444, 0.6376488095238095, 0.694375, 0.5952380952380951, 0.5217013888888888, 0.4494047619047619, 0.6307043650793651, nan, 0.6643105158730158, 0.7222222222222223, 0.6476934523809523, 0.7291666666666666, nan, 0.7078786375661374, 0.6982275132275132, 0.4294394841269842, 0.4654017857142857, 0.4117063492063492, nan, nan, 0.5325520833333334, 0.6433597883597884, 0.44921875, 0.6638144841269842, 0.6428571428571428, 0.7044270833333334, 0.6910962301587302, 0.415922619047619, 0.7005208333333333, 0.5256489748677249, 0.4279100529100529, 0.46019345238095244, 0.8392857142857143, 0.4854910714285714, nan, 0.6986607142857142, nan, 0.6063988095238095, 0.4913814484126984, 0.5531994047619048, 0.6067708333333334, 0.4947916666666667, 0.3492708333333333, 0.7829861111111112, 0.5669047619047619, 0.5911458333333333, 0.5695684523809523, 0.6882440476190476, 0.6028645833333333, 0.7258184523809523, 0.6116071428571428, 0.5171130952380952, 0.4667245370370371, 0.8015873015873015, 0.8902529761904762, 0.5033068783068784, 0.6372767857142858, nan, 0.5509672619047619, nan, 0.5825892857142857, 0.5452628968253967, 0.6495535714285714, 0.609375, 0.8055555555555556, 0.7240277777777777, 0.7465277777777778, 0.4701041666666667, 0.5682010582010582, 0.6369460978835979, 0.6393229166666667, nan, 0.7516402116402116, 0.5769047619047619, 0.6333068783068783, nan, nan, 0.80078125, 0.34656084656084657, 0.5670833333333334, 0.4465625, 0.5788690476190477, 0.6753472222222222, 0.652048611111111, 0.7133969907407406, 0.6362698412698413, 0.6447172619047619, 0.5777529761904762, 0.5200892857142857, 0.6183035714285714, 0.42180555555555554, 0.9642857142857142, 0.5316220238095238, 0.6538888888888889, nan, 0.5572916666666666, 0.7008928571428571, nan, 0.6655505952380953, nan, 0.27326388888888886, 0.5231068121693122, 0.8303125, 0.7135416666666666, 0.38721478174603174, 0.5532539682539683, 0.72265625, 0.7075892857142856, 0.527281746031746, 0.6790624999999999, 0.6145833333333333, 0.8905629960317459, 0.7409027777777777, 0.7447916666666666, 0.7209375, 0.4340277777777777, 0.7390873015873016, 0.6090029761904763, 0.4333664021164021, 0.4035714285714286, 0.6380208333333334, 0.6837797619047619, 0.6805555555555556, 0.7271875, 0.61328125, 0.6439732142857142, nan, 0.6506696428571428, 0.7925347222222222, 0.3863673941798942, 0.5928199404761905, 0.319047619047619, 0.5691550925925926, 0.878968253968254, 0.5924652777777778, 0.6361607142857143, 0.5602678571428571, 0.5628306878306878, 0.6884920634920635, 0.5422222222222222, 0.7905505952380952, 0.4613095238095238, 0.5669642857142856, 0.5999917328042328, 0.36607142857142855, 0.5259375, 0.7848048941798942, 0.5987103174603174, 0.43266369047619047, 0.6947544642857143, nan, 0.7041790674603174, 0.5763888888888888, 0.5941220238095237, 0.5089285714285714, nan, 0.4324570105820106, 0.8273809523809523, 0.43749999999999994, 0.6341352513227513, 0.5476190476190476, 0.6593915343915344, 0.5818253968253968, 0.6294642857142856, 0.7038690476190477, 0.7113095238095237, 0.4408068783068783, 0.7317708333333333, 0.8410416666666667, 0.6312003968253967, 0.519345238095238, 0.6045386904761905, 0.5414186507936507, nan, 0.4174107142857143, 0.4548611111111111, 0.5037202380952381, 0.7436755952380952, nan, 0.7565104166666666, 0.5591534391534392, 0.47371031746031744, 0.613095238095238, 0.5926339285714286, 0.6888020833333333, 0.5266617063492063, nan, 0.6960416666666667, 0.5259796626984127, nan, nan, 0.7418154761904762, 0.42968749999999994, 0.4401041666666667, 0.41015625, 0.5634920634920635, 0.7064732142857143, 0.6850818452380952, 0.6673280423280423, 0.5902777777777778, 0.69140625, 0.6276041666666666, 0.5833333333333334, 0.6760912698412699, 0.4195833333333333, nan, 0.6517857142857143, 0.646701388888889, 0.5364583333333333, 0.5427827380952381, 0.5535714285714286, 0.7777248677248677, 0.5694444444444444, 0.6114004629629629, 0.3104745370370371, 0.5095502645502645, 0.6730158730158731, 0.760132275132275, nan, nan, 0.6892361111111112, 0.7131076388888888, 0.5490625, 0.724375, 0.4486607142857143, 0.8194444444444444, 0.805625, 0.7001488095238095, 0.762910052910053, 0.4598214285714286, 0.6815476190476191, 0.6049107142857142, 0.3504464285714286, 0.41218750000000004, 0.6785714285714286, 0.6696428571428572, 0.7872222222222223, nan, 0.6726190476190477, 0.5814732142857142, nan, 0.5684523809523809, nan, 0.37760416666666663, 0.5598338293650793, 0.36375, 0.43576388888888884, 0.8621031746031746, 0.733095238095238, 0.5091145833333333, 0.5877976190476191, 0.4986359126984127, 0.6969791666666667, 0.6015625, 0.7860863095238095, 0.3496875, 0.6002604166666666, 0.4769791666666667, 0.47817460317460314, 0.41579861111111105, 0.6097470238095237, 0.5264136904761905, 0.5042857142857143, 0.6551339285714286, 0.5572916666666666, 0.667989417989418, 0.5677083333333334, 0.5924479166666666, 0.5, nan, 0.3523065476190476, 0.9017857142857143, 0.42838541666666663, 0.6949404761904763, 0.5133928571428572, 0.6209375, 0.6575520833333334, 0.5998263888888888, 0.81375, 0.6770833333333333, 0.45545138888888886, 0.6768353174603174, 0.9257192460317459, 0.777157738095238, 0.4267113095238095, 0.6135714285714287, 0.6428571428571428, 0.3359375, 0.3656349206349206, 0.40576388888888887, 0.6276041666666667, 0.7526041666666666, nan, 0.7633928571428572, 0.6886160714285714, 0.4975198412698413, 0.5598958333333334, 0.5652281746031745, 0.7278645833333333, 0.5714285714285714, nan, 0.7607886904761905, 0.4310515873015873, nan, nan, 0.68359375, 0.5749470899470899, 0.421875, 0.5245535714285714, 0.5133928571428572, 0.66796875, 0.674045138888889, 0.7016369047619048, 0.6517857142857143, 0.7944155092592593, 0.6104497354497354, 0.5803571428571428, 0.6190476190476191, 0.4188988095238095, nan, 0.7079613095238095, nan, 0.5993303571428571, 0.4358878968253968, 0.6480654761904762, 0.65234375, 0.5017361111111112, 0.8571527777777778, 0.3735532407407407, nan, 0.5407291666666667, 0.8443287037037036] AUC: 0.8443287037037036 Test AUC_Std: nan \n",
            "\n",
            "(Sat Apr 18 14:53:49 2020) Epoch:  2  Training Cost:  1.3855228574932656 Training Accuracy:  0.26253102264984074\n",
            "(Sat Apr 18 14:53:57 2020) Epoch:  2 Test Cost:  1.385763329844321 \n",
            " Test Accuracy:  0.2657568291310341 Test Acc_Std: 0.14148665080948178 \n",
            " Test Precision: 0.1660660128402064 Test Pre_Std: 0.13013636853280047 \n",
            " Test Recall_score: 0.25699958643507026 Test Rec_Std: 0.12697772521053483 \n",
            " Test f1:  0.1702058831711437 Test f1_Std: 0.10265793630416514 \n",
            " Test AUC:  nan auc_roc_test:  [0.24144345238095238, 0.5679976851851852, 0.37991898148148145, 0.4908854166666667, 0.5522486772486772, 0.8482142857142857, 0.8350694444444444, 0.5886243386243386, 0.5634920634920635, 0.46484375, 0.7447916666666666, 0.6875000000000001, 0.4613715277777778, 0.5261458333333333, nan, 0.5993303571428571, 0.5442088293650793, 0.6313244047619047, 0.6104910714285714, 0.7306547619047619, 0.6373809523809524, 0.6624503968253967, 0.6545138888888888, 0.7115162037037037, 0.7407291666666667, 0.7907986111111112, 0.6091269841269842, 0.6549479166666667, nan, 0.6642485119047619, 0.5826041666666666, 0.6666666666666666, 0.6477347883597884, 0.6793650793650794, 0.8268229166666667, 0.4386160714285714, 0.7922867063492063, 0.4264583333333334, 0.80078125, 0.6851851851851852, 0.6259375, 0.70703125, 0.6492013888888889, 0.5641121031746031, 0.6823536706349207, 0.6153273809523809, 0.6161541005291005, 0.4115079365079365, 0.8013392857142857, 0.5658482142857142, 0.4222222222222222, 0.5924305555555556, 0.6796875, 0.6116071428571428, nan, 0.6830357142857143, 0.5047123015873015, 0.478546626984127, 0.4542410714285714, 0.626031746031746, 0.3743179563492064, 0.5680952380952381, 0.7452083333333333, 0.5978422619047619, 0.7094494047619047, 0.6964285714285714, 0.46097883597883593, 0.6088624338624338, 0.6588541666666665, 0.6845238095238095, 0.7101934523809524, 0.6513310185185186, 0.7072172619047619, 0.7953125, 0.5524553571428571, 0.6041666666666666, 0.5001240079365079, 0.4379340277777778, nan, 0.5510912698412699, 0.7413194444444444, 0.6770833333333334, 0.5528273809523809, nan, 0.6793981481481481, 0.5852116402116401, 0.5233961640211641, 0.5066964285714286, 0.5894923941798942, nan, nan, 0.78125, 0.6820370370370371, 0.56640625, 0.7440476190476191, 0.7239583333333333, 0.4375, 0.6015625, 0.3962053571428571, 0.8143601190476191, 0.7118055555555556, 0.4801587301587302, 0.5375744047619048, 0.738095238095238, 0.5844494047619048, nan, 0.35788690476190477, nan, 0.5565476190476191, 0.365327380952381, 0.5066964285714285, 0.578125, 0.5086805555555556, 0.5443055555555556, 0.6357060185185186, 0.40074074074074073, 0.53125, 0.5497271825396826, 0.5212053571428572, 0.8216145833333333, 0.8201884920634921, 0.6737351190476191, 0.7038690476190476, 0.3819444444444444, 0.6911375661375662, 0.7697172619047619, 0.539021164021164, 0.4174107142857143, nan, 0.5334821428571428, nan, 0.7310267857142856, 0.5411706349206349, 0.6413690476190476, 0.65625, 0.7638888888888888, 0.5893055555555555, 0.7980324074074074, 0.6469791666666667, 0.858042328042328, 0.5758928571428572, 0.7200520833333333, nan, 0.6074074074074074, 0.603015873015873, 0.5471693121693121, nan, nan, 0.8043154761904762, 0.3663194444444444, 0.7073263888888889, 0.5238888888888888, 0.6837797619047619, 0.7465277777777777, 0.6492013888888889, 0.6303943452380952, 0.643015873015873, 0.6290922619047619, 0.6130952380952381, 0.7536168981481481, 0.49441964285714285, 0.41128472222222223, 0.7599206349206349, 0.6529017857142857, 0.33928571428571425, nan, 0.5621279761904762, 0.6354166666666666, nan, 0.6741071428571428, nan, 0.5361805555555555, 0.5127314814814814, 0.526701388888889, 0.38020833333333337, 0.6302083333333333, 0.7622751322751323, 0.6809895833333334, 0.5323660714285714, 0.6979166666666666, 0.49437499999999995, 0.5638020833333333, 0.7445436507936508, 0.6165624999999999, 0.703125, 0.7432291666666666, 0.6006944444444444, 0.8148561507936509, 0.5982142857142857, 0.6514136904761905, 0.4896825396825397, 0.613095238095238, 0.5621279761904762, 0.5905555555555555, 0.6915624999999999, 0.6783854166666666, 0.503720238095238, nan, 0.6246279761904762, 0.6068948412698413, 0.5145709325396826, 0.6390335648148148, 0.4661111111111111, 0.6622023809523809, 0.6293121693121693, 0.6960069444444444, 0.7503720238095238, 0.6733010912698412, 0.6646825396825397, 0.7076719576719576, 0.38965608465608464, 0.6398809523809524, 0.40885416666666663, 0.6990327380952381, 0.7212301587301587, 0.41927083333333326, 0.4070833333333333, 0.6201636904761905, 0.5659722222222222, 0.7101934523809524, 0.3374669312169312, nan, 0.73046875, 0.3107638888888889, 0.5978422619047619, 0.6067708333333333, nan, 0.5319320436507937, 0.6686243386243386, 0.24367559523809523, 0.5724619708994708, 0.4900793650793651, 0.57010582010582, 0.755978835978836, 0.8180803571428572, 0.6748511904761905, 0.8377976190476191, 0.3737599206349207, 0.6037946428571428, 0.7455208333333334, 0.6220238095238095, 0.3073743386243386, 0.6369047619047619, 0.6671213624338624, nan, 0.748077876984127, 0.3993055555555556, 0.6819196428571429, 0.828125, nan, 0.546875, 0.6907936507936507, 0.6553819444444444, 0.8329199735449735, 0.7116815476190477, 0.55078125, 0.6091269841269841, nan, 0.6778124999999999, 0.5104166666666667, nan, nan, 0.7607886904761904, 0.5775462962962963, 0.5474537037037037, 0.6458333333333334, 0.58994708994709, 0.7090773809523809, 0.7669270833333334, 0.6362433862433862, 0.6382688492063493, 0.6276041666666666, 0.609375, 0.6614583333333334, 0.5913525132275133, 0.4725347222222222, nan, 0.8024553571428572, 0.30704365079365076, 0.5967261904761905, 0.6201636904761905, 0.6335565476190477, 0.6582275132275133, 0.722635582010582, 0.6886574074074073, 0.5182291666666666, 0.4732010582010582, 0.601005291005291, 0.4285714285714286, nan, nan, 0.6832837301587301, 0.5877976190476191, 0.5339583333333333, 0.4140625, 0.6030505952380951, 0.7743055555555556, 0.619375, 0.6390128968253969, 0.8724867724867724, 0.6164434523809523, 0.7198660714285715, 0.5880456349206349, 0.6056547619047619, 0.35034722222222225, 0.6316137566137566, 0.5301339285714286, 0.799047619047619, nan, 0.6781994047619048, 0.6060267857142857, nan, 0.6067708333333333, nan, 0.5754513888888888, 0.7116815476190477, 0.46020833333333333, 0.6684027777777779, 0.5925099206349207, 0.7440476190476191, 0.5338541666666666, 0.6986607142857142, 0.5734126984126984, 0.6967708333333333, 0.6015625, 0.6262400793650794, 0.3414236111111111, 0.6119791666666667, 0.8215277777777777, 0.44055886243386244, 0.3917824074074074, 0.5528273809523809, 0.5236855158730158, 0.7897883597883598, 0.36793154761904756, 0.5059523809523809, 0.703968253968254, 0.2907638888888889, 0.58203125, 0.5651041666666666, nan, 0.5256696428571428, 0.636656746031746, 0.2747395833333333, 0.5145089285714285, 0.4747643849206349, 0.58375, 0.6549479166666667, 0.7803819444444444, 0.848125, 0.7252604166666666, 0.4646180555555555, 0.4587053571428572, 0.7317708333333333, 0.7194940476190477, 0.3514384920634921, 0.5708994708994708, 0.7626488095238095, 0.6216517857142857, 0.6251587301587301, 0.4984375, 0.7057291666666666, 0.839657738095238, nan, 0.5052083333333333, 0.7574404761904762, 0.7134176587301587, 0.7309027777777778, 0.7265625, 0.5989583333333334, 0.5978422619047619, nan, 0.6558779761904762, 0.5011160714285714, nan, nan, 0.7057291666666667, 0.8369047619047619, 0.3697916666666667, 0.77734375, 0.6421130952380952, 0.7486979166666666, 0.7492559523809523, 0.6640625, 0.6324404761904762, 0.5518973214285714, 0.6825396825396827, 0.7209821428571428, 0.5152116402116402, 0.5130208333333334, nan, 0.8035714285714286, nan, 0.4564732142857143, 0.46688988095238093, 0.7719494047619049, 0.60546875, 0.6197916666666666, 0.7517013888888888, 0.4835069444444444, nan, 0.5311111111111111, 0.5654555224867726] AUC: 0.5654555224867726 Test AUC_Std: nan \n",
            "\n",
            "(Sat Apr 18 14:54:16 2020) Epoch:  3  Training Cost:  1.3878235849494083 Training Accuracy:  0.25012407479599746\n",
            "(Sat Apr 18 14:54:25 2020) Epoch:  3 Test Cost:  1.3886717640732418 \n",
            " Test Accuracy:  0.24863524097145048 Test Acc_Std: 0.12065525801185664 \n",
            " Test Precision: 0.06441273779983459 Test Pre_Std: 0.03313505599665102 \n",
            " Test Recall_score: 0.25392886683209265 Test Rec_Std: 0.04194576055676666 \n",
            " Test f1:  0.0993408817540331 Test f1_Std: 0.04228349193934903 \n",
            " Test AUC:  nan auc_roc_test:  [0.6398809523809523, 0.7986111111111112, 0.423900462962963, 0.6484375, 0.4365079365079365, 0.7131696428571428, 0.7564484126984127, 0.6329365079365079, 0.675595238095238, 0.5611979166666667, 0.6184895833333333, 0.6640625, 0.5142815806878307, 0.6384027777777778, nan, 0.39322916666666663, 0.5252976190476191, 0.638764880952381, 0.7094494047619047, 0.8039434523809523, 0.6014021164021164, 0.5890997023809523, 0.556423611111111, 0.631076388888889, 0.7695138888888888, 0.6770833333333334, 0.5588624338624337, 0.7265625, nan, 0.6067708333333333, 0.32538194444444446, 0.4913194444444444, 0.7152777777777778, 0.6024867724867725, 0.8294270833333335, 0.48065476190476186, 0.6037739748677249, 0.7021875, 0.8528645833333334, 0.6547619047619048, 0.6152777777777778, 0.7552083333333334, 0.6219791666666666, 0.5831679894179894, 0.7084986772486773, 0.5290178571428571, 0.3907490079365079, 0.6954232804232804, 0.7083333333333334, 0.6298363095238095, 0.49550264550264556, 0.46927083333333336, 0.6067708333333333, 0.6536458333333333, nan, 0.5669642857142857, 0.5627480158730158, 0.48852926587301587, 0.4419642857142857, 0.5914550264550265, 0.5088045634920635, 0.5728306878306878, 0.8137847222222222, 0.6127232142857143, 0.6622023809523809, 0.7420634920634921, 0.6124338624338624, 0.6316137566137566, 0.6454613095238095, 0.5691964285714286, 0.7760416666666666, 0.5622933201058202, 0.2883184523809524, 0.5695833333333334, 0.49913194444444453, 0.6953125, 0.5221354166666666, 0.4305555555555555, nan, 0.7022569444444444, 0.6944444444444444, 0.6510416666666666, 0.5595238095238095, nan, 0.6297123015873016, 0.4434920634920635, 0.4204282407407407, 0.6502976190476191, 0.4651537698412699, nan, nan, 0.8111979166666666, 0.5611111111111111, 0.65625, 0.5234375, 0.7265625, 0.4661458333333333, 0.7292906746031745, 0.3597470238095237, 0.6395089285714285, 0.7292286706349206, 0.6190476190476191, 0.6785714285714285, 0.6025132275132276, 0.7150297619047619, nan, 0.49441964285714285, nan, 0.6651785714285714, 0.5865162037037037, 0.6097470238095238, 0.62890625, 0.765625, 0.5430208333333333, 0.6414930555555556, 0.84, 0.4088541666666667, 0.470651455026455, 0.6651785714285714, 0.66796875, 0.6794394841269842, 0.6863839285714285, 0.816220238095238, 0.48828125, 0.7003968253968254, 0.6082589285714286, 0.5436507936507936, 0.5587797619047619, nan, 0.3470982142857143, nan, 0.6395089285714286, 0.65625, 0.6915922619047619, 0.5833333333333334, 0.6649305555555556, 0.6215625, 0.43778935185185186, 0.7631249999999999, 0.6792857142857143, 0.607452876984127, 0.7408854166666667, nan, 0.6051587301587302, 0.49089947089947084, 0.34973544973544973, nan, nan, 0.9088541666666666, 0.28373015873015867, 0.7804861111111111, 0.73, 0.7005208333333334, 0.7100694444444444, 0.7409375, 0.7327628968253967, 0.6105026455026454, 0.5807291666666666, 0.6205357142857143, 0.6051587301587302, 0.3630952380952381, 0.6310416666666667, 0.7301587301587301, 0.7488839285714286, 0.38267195767195766, nan, 0.5390625, 0.5710565476190476, nan, 0.447172619047619, nan, 0.48038194444444443, 0.482514880952381, 0.6459722222222222, 0.43576388888888884, 0.564298115079365, 0.7932804232804234, 0.7252604166666666, 0.5234375, 0.6830357142857142, 0.6602777777777777, 0.640625, 0.6717509920634921, 0.5761111111111111, 0.6653645833333334, 0.6426736111111111, 0.32258597883597884, 0.5248015873015873, 0.49962797619047616, 0.6703869047619048, 0.4519576719576719, 0.5550595238095238, 0.5967261904761905, 0.7118518518518518, 0.7367361111111111, 0.6692708333333334, 0.47953869047619047, nan, 0.6264880952380952, 0.47693452380952384, 0.4447337962962963, 0.6298363095238095, 0.5334126984126984, 0.5685143849206349, 0.5987301587301587, 0.7745486111111111, 0.5863095238095238, 0.689360119047619, 0.47949735449735453, 0.746031746031746, 0.4726719576719577, 0.6819196428571428, 0.42373511904761907, 0.5405505952380951, 0.8203125, 0.5628720238095237, 0.6142013888888889, 0.560763888888889, 0.621073082010582, 0.7291666666666666, 0.5486111111111112, nan, 0.8623511904761905, 0.5850694444444444, 0.6220238095238095, 0.6889880952380952, nan, 0.627170138888889, 0.6486507936507937, 0.7183779761904762, 0.4301215277777778, 0.5046296296296297, 0.8214285714285714, 0.622063492063492, 0.7269345238095237, 0.5636160714285714, 0.9404761904761905, 0.4308035714285714, 0.5837053571428571, 0.6237152777777778, 0.6049107142857143, 0.5081431878306879, 0.4888392857142857, 0.46168154761904756, nan, 0.660342261904762, 0.5607638888888888, 0.7433035714285714, 0.6700148809523809, nan, 0.6414930555555555, 0.5211904761904762, 0.65625, 0.7313988095238095, 0.5338541666666666, 0.7265625, 0.611400462962963, nan, 0.5179861111111111, 0.3851066468253968, nan, nan, 0.8883928571428572, 0.4045138888888889, 0.640625, 0.7291666666666666, 0.625, 0.8307291666666667, 0.6522817460317459, 0.6038359788359788, 0.6536458333333334, 0.6302083333333333, 0.61328125, 0.6236979166666667, 0.4385954034391535, 0.6223611111111111, nan, 0.7704613095238095, 0.44122023809523814, 0.41666666666666663, 0.5267857142857142, 0.5866815476190476, 0.701031746031746, 0.49789186507936506, 0.515625, 0.5868055555555556, 0.46084656084656084, 0.6399999999999999, 0.4717989417989418, nan, nan, 0.7359871031746031, 0.5776289682539683, 0.5964930555555555, 0.6145833333333334, 0.6863839285714285, 0.7361111111111112, 0.5509722222222222, 0.6846064814814814, 0.7398148148148147, 0.41108630952380953, 0.45535714285714285, 0.48832258597883593, 0.6488095238095238, 0.5160069444444444, 0.5681216931216931, 0.597470238095238, 0.6134920634920635, nan, 0.5725446428571428, 0.5119047619047619, nan, 0.6834077380952381, nan, 0.4615625, 0.5949900793650794, 0.4465625, 0.4774305555555556, 0.5815972222222222, 0.6518253968253969, 0.625, 0.5944940476190477, 0.4614955357142857, 0.5784374999999999, 0.6809895833333333, 0.6738591269841269, 0.3451388888888889, 0.5625, 0.5281250000000001, 0.5089285714285714, 0.6372767857142857, 0.5967261904761905, 0.5756448412698413, 0.7875132275132275, 0.5718005952380952, 0.3854166666666667, 0.8512169312169311, 0.4351041666666666, 0.6197916666666667, 0.6361607142857142, nan, 0.5658482142857143, 0.6240079365079365, 0.6979166666666667, 0.40587797619047616, 0.4620535714285714, 0.8518749999999999, 0.4921875, 0.7652943121693121, 0.5673958333333333, 0.9140625, 0.5399999999999999, 0.49807787698412703, 0.6157614087301587, 0.6450892857142857, 0.3349454365079365, 0.579973544973545, 0.5219494047619048, 0.5264136904761905, 0.5334126984126984, 0.5389930555555555, 0.8268229166666666, 0.7124255952380952, nan, 0.5892857142857143, 0.6194196428571429, 0.5714285714285714, 0.6866319444444444, 0.6103257275132274, 0.59375, 0.6618303571428572, nan, 0.6246279761904762, 0.35466269841269843, nan, nan, 0.80859375, 0.5860846560846561, 0.4231770833333333, 0.7849082341269841, 0.7273065476190476, 0.8841145833333334, 0.574156746031746, 0.65625, 0.613095238095238, 0.521515376984127, 0.7797619047619047, 0.5915178571428571, 0.4054232804232804, 0.6294642857142858, nan, 0.7217261904761905, nan, 0.4181547619047619, 0.4626322751322751, 0.5803571428571428, 0.7265625, 0.5121527777777778, 0.6493055555555555, 0.5260416666666666, nan, 0.5029166666666667, 0.5190972222222222] AUC: 0.5190972222222222 Test AUC_Std: nan \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHvxN0gvGL9p",
        "colab_type": "code",
        "outputId": "95e57593-0bff-4c56-a23e-eb43716bb898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#! /usr/bin/python3\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "#from cnn_class import cnn\n",
        "import time\n",
        "import scipy.io as sio\n",
        "from sklearn.metrics import classification_report, roc_auc_score, auc, roc_curve, f1_score, precision_score, \\\n",
        "    recall_score\n",
        "#from RnnAttention.attention import attention\n",
        "from scipy import interp\n",
        "\n",
        "\n",
        "def multiclass_roc_auc_score(y_true, y_score):\n",
        "    assert y_true.shape == y_score.shape\n",
        "    fpr = dict()\n",
        "    tpr = dict()\n",
        "    roc_auc = dict()\n",
        "    n_classes = y_true.shape[1]\n",
        "    # compute ROC curve and ROC area for each class\n",
        "    for i in range(n_classes):\n",
        "        fpr[i], tpr[i], _ = roc_curve(y_true[:, i], y_score[:, i])\n",
        "        roc_auc[i] = auc(fpr[i], tpr[i])\n",
        "    # compute micro-average ROC curve and ROC area\n",
        "    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_true.ravel(), y_score.ravel())\n",
        "    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n",
        "\n",
        "    # compute macro-average ROC curve and ROC area\n",
        "    # First aggregate all false probtive rates\n",
        "    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n",
        "    # Then interpolate all ROC curves at this points\n",
        "    mean_tpr = np.zeros_like(all_fpr)\n",
        "    for i in range(n_classes):\n",
        "        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n",
        "    # Finally average it and compute AUC\n",
        "    mean_tpr /= n_classes\n",
        "    fpr[\"macro\"] = all_fpr\n",
        "    tpr[\"macro\"] = mean_tpr\n",
        "    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n",
        "    return roc_auc\n",
        "\n",
        "\n",
        "###########################################################################\n",
        "# prepare raw data\n",
        "###########################################################################\n",
        "subject_id = 1\n",
        "data_folder = '/content/drive/My Drive/Colab Notebooks/'\n",
        "data = sio.loadmat(data_folder+\"/5_cross_sub/cross_subject_data_\"+str(subject_id)+\".mat\")\n",
        "print(\"subject id \", subject_id)\n",
        "\n",
        "test_X = data[\"test_x\"]  # [trials, channels, time length]\n",
        "train_X = data[\"train_x\"]\n",
        "\n",
        "test_y = data[\"test_y\"].ravel()\n",
        "train_y = data[\"train_y\"].ravel()\n",
        "\n",
        "train_y = np.asarray(pd.get_dummies(train_y), dtype=np.int8)\n",
        "test_y = np.asarray(pd.get_dummies(test_y), dtype=np.int8)\n",
        "\n",
        "###########################################################################\n",
        "# crop data\n",
        "###########################################################################\n",
        "\n",
        "window_size = 400\n",
        "step = 50\n",
        "n_channel = 22\n",
        "\n",
        "def windows(data, size, step):\n",
        "    start = 0\n",
        "    while ((start + size) < data.shape[0]):\n",
        "        yield int(start), int(start + size)\n",
        "        start += step\n",
        "\n",
        "\n",
        "def segment_signal_without_transition(data, window_size, step):\n",
        "    segments = []\n",
        "    for (start, end) in windows(data, window_size, step):\n",
        "        if (len(data[start:end]) == window_size):\n",
        "            segments = segments + [data[start:end]]\n",
        "    return np.array(segments)\n",
        "\n",
        "\n",
        "def segment_dataset(X, window_size, step):\n",
        "    win_x = []\n",
        "    for i in range(X.shape[0]):\n",
        "        win_x = win_x + [segment_signal_without_transition(X[i], window_size, step)]\n",
        "    win_x = np.array(win_x)\n",
        "    return win_x\n",
        "\n",
        "train_raw_x = np.transpose(train_X, [0, 2, 1])\n",
        "test_raw_x = np.transpose(test_X, [0, 2, 1])\n",
        "\n",
        "train_win_x = segment_dataset(train_raw_x, window_size, step)\n",
        "print(\"train_win_x shape: \", train_win_x.shape)\n",
        "test_win_x = segment_dataset(test_raw_x, window_size, step)\n",
        "print(\"test_win_x shape: \", test_win_x.shape)\n",
        "\n",
        "# [trial, window, channel, time_length]\n",
        "train_win_x = np.transpose(train_win_x, [0, 1, 3, 2])\n",
        "print(\"train_win_x shape: \", train_win_x.shape)\n",
        "\n",
        "test_win_x = np.transpose(test_win_x, [0, 1, 3, 2])\n",
        "print(\"test_win_x shape: \", test_win_x.shape)\n",
        "\n",
        "# [trial, window, channel, time_length, 1]\n",
        "train_x = np.expand_dims(train_win_x, axis=4)\n",
        "test_x = np.expand_dims(test_win_x, axis=4)\n",
        "\n",
        "num_timestep = train_x.shape[1]\n",
        "###########################################################################\n",
        "# set model parameters\n",
        "###########################################################################\n",
        "# kernel parameter\n",
        "kernel_height_1st = 22\n",
        "kernel_width_1st = 45\n",
        "\n",
        "kernel_stride = 1\n",
        "\n",
        "conv_channel_num = 40\n",
        "\n",
        "# pooling parameter\n",
        "pooling_height_1st = 1\n",
        "pooling_width_1st = 75\n",
        "\n",
        "pooling_stride_1st = 10\n",
        "\n",
        "# full connected parameter\n",
        "attention_size = 512\n",
        "n_hidden_state = 64\n",
        "\n",
        "###########################################################################\n",
        "# set dataset parameters\n",
        "###########################################################################\n",
        "# input channel\n",
        "input_channel_num = 1\n",
        "\n",
        "# input height \n",
        "input_height = train_x.shape[2]\n",
        "\n",
        "# input width\n",
        "input_width = train_x.shape[3]\n",
        "\n",
        "# prediction class\n",
        "num_labels = 4\n",
        "###########################################################################\n",
        "# set training parameters\n",
        "###########################################################################\n",
        "# set learning rate\n",
        "learning_rate = 1e-4\n",
        "\n",
        "# set maximum traing epochs\n",
        "training_epochs = 3\n",
        "\n",
        "# set batch size\n",
        "batch_size = 10\n",
        "\n",
        "# set dropout probability\n",
        "dropout_prob = 0.5\n",
        "\n",
        "# set train batch number per epoch\n",
        "batch_num_per_epoch = train_x.shape[0] // batch_size\n",
        "\n",
        "# instance cnn class\n",
        "padding = 'VALID'\n",
        "\n",
        "cnn_2d = cnn(padding=padding)\n",
        "\n",
        "# input placeholder\n",
        "X = tf.placeholder(tf.float32, shape=[None, input_height, input_width, input_channel_num], name='X')\n",
        "Y = tf.placeholder(tf.float32, shape=[None, num_labels], name='Y')\n",
        "train_phase = tf.placeholder(tf.bool, name='train_phase')\n",
        "keep_prob = tf.placeholder(tf.float32, name='keep_prob')\n",
        "\n",
        "# first CNN layer\n",
        "conv_1 = cnn_2d.apply_conv2d(X, kernel_height_1st, kernel_width_1st, input_channel_num, conv_channel_num, kernel_stride,\n",
        "                             train_phase)\n",
        "print(\"conv 1 shape: \", conv_1.get_shape().as_list())\n",
        "pool_1 = cnn_2d.apply_max_pooling(conv_1, pooling_height_1st, pooling_width_1st, pooling_stride_1st)\n",
        "print(\"pool 1 shape: \", pool_1.get_shape().as_list())\n",
        "\n",
        "pool1_shape = pool_1.get_shape().as_list()\n",
        "pool1_flat = tf.reshape(pool_1, [-1, pool1_shape[1] * pool1_shape[2] * pool1_shape[3]])\n",
        "\n",
        "fc_drop = tf.nn.dropout(pool1_flat, keep_prob)\n",
        "\n",
        "lstm_in = tf.reshape(fc_drop, [-1, num_timestep, pool1_shape[1] * pool1_shape[2] * pool1_shape[3]])\n",
        "\n",
        "########################## RNN ########################\n",
        "cells = []\n",
        "for _ in range(2):\n",
        "    cell = tf.contrib.rnn.BasicLSTMCell(n_hidden_state, forget_bias=1.0, state_is_tuple=True)\n",
        "    cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
        "    cells.append(cell)\n",
        "lstm_cell = tf.contrib.rnn.MultiRNNCell(cells)\n",
        "\n",
        "init_state = lstm_cell.zero_state(batch_size, dtype=tf.float32)\n",
        "\n",
        "# output ==> [batch, step, n_hidden_state]\n",
        "rnn_op, states = tf.nn.dynamic_rnn(lstm_cell, lstm_in, initial_state=init_state, time_major=False)\n",
        "\n",
        "########################## attention ########################\n",
        "with tf.name_scope('Attention_layer'):\n",
        "    attention_op, alphas = attention(rnn_op, attention_size, time_major=False, return_alphas=True)\n",
        "\n",
        "attention_drop = tf.nn.dropout(attention_op, keep_prob)\n",
        "\n",
        "########################## readout ########################\n",
        "y_ = cnn_2d.apply_readout(attention_drop, rnn_op.shape[2].value, num_labels)\n",
        "\n",
        "# probability prediction \n",
        "y_prob = tf.nn.softmax(y_, name=\"y_prob\")\n",
        "\n",
        "# class prediction \n",
        "y_pred = tf.argmax(y_prob, 1, name=\"y_pred\")\n",
        "\n",
        "########################## loss and optimizer ########################\n",
        "# cross entropy cost function\n",
        "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=y_, labels=Y), name='loss')\n",
        "\n",
        "update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
        "with tf.control_dependencies(update_ops):\n",
        "    # set training SGD optimizer\n",
        "    optimizer = tf.train.AdamOptimizer(learning_rate).minimize(cost)\n",
        "\n",
        "# get correctly predicted object\n",
        "correct_prediction = tf.equal(tf.argmax(tf.nn.softmax(y_), 1), tf.argmax(Y, 1))\n",
        "\n",
        "########################## define accuracy ########################\n",
        "# calculate prediction accuracy\n",
        "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32), name='accuracy')\n",
        "\n",
        "###########################################################################\n",
        "# train test and save result\n",
        "###########################################################################\n",
        "\n",
        "# run with gpu memory growth\n",
        "config = tf.ConfigProto()\n",
        "config.gpu_options.allow_growth = True\n",
        "\n",
        "train_acc = []\n",
        "test_acc = []\n",
        "best_test_acc = []\n",
        "train_loss = []\n",
        "\n",
        "cost_arr = []\n",
        "accuracy_arr = []\n",
        "f1_arr = []\n",
        "auc_arr = []\n",
        "precision_arr = []\n",
        "recall_arr = []\n",
        "\n",
        "\n",
        "with tf.Session(config=config) as session:\n",
        "    session.run(tf.global_variables_initializer())\n",
        "    best_acc = 0\n",
        "\n",
        "    for epoch in range(training_epochs):\n",
        "        pred_test = np.array([])\n",
        "        true_test = []\n",
        "        prob_test = []\n",
        "\n",
        "\n",
        "        ########################## training process ########################\n",
        "        for b in range(batch_num_per_epoch):\n",
        "            offset = (b * batch_size) % (train_y.shape[0] - batch_size)\n",
        "            batch_x = train_x[offset:(offset + batch_size), :, :, :, :]\n",
        "            batch_x = batch_x.reshape([len(batch_x) * num_timestep, n_channel, window_size, 1])\n",
        "            batch_y = train_y[offset:(offset + batch_size), :]\n",
        "            _, c = session.run([optimizer, cost],\n",
        "                               feed_dict={X: batch_x, Y: batch_y, keep_prob: 1 - dropout_prob, train_phase: True})\n",
        "        # calculate train and test accuracy after each training epoch\n",
        "        if (epoch % 1 == 0):\n",
        "            train_accuracy = np.zeros(shape=[0], dtype=float)\n",
        "            test_accuracy = np.zeros(shape=[0], dtype=float)\n",
        "            train_l = np.zeros(shape=[0], dtype=float)\n",
        "            test_l = np.zeros(shape=[0], dtype=float)\n",
        "            # calculate train accuracy after each training epoch\n",
        "            for i in range(batch_num_per_epoch):\n",
        "                ########################## prepare training data ########################\n",
        "                offset = (i * batch_size) % (train_y.shape[0] - batch_size)\n",
        "                train_batch_x = train_x[offset:(offset + batch_size), :, :, :]\n",
        "                train_batch_x = train_batch_x.reshape([len(train_batch_x) * num_timestep, n_channel, window_size, 1])\n",
        "                train_batch_y = train_y[offset:(offset + batch_size), :]\n",
        "\n",
        "                ########################## calculate training results ########################\n",
        "                train_a, train_c = session.run([accuracy, cost],\n",
        "                                               feed_dict={X: train_batch_x, Y: train_batch_y, keep_prob: 1.0,\n",
        "                                                          train_phase: False})\n",
        "\n",
        "                train_l = np.append(train_l, train_c)\n",
        "                train_accuracy = np.append(train_accuracy, train_a)\n",
        "            print(\"(\" + time.asctime(time.localtime(time.time())) + \") Epoch: \", epoch + 1, \" Training Cost: \",\n",
        "                  np.mean(train_l), \"Training Accuracy: \", np.mean(train_accuracy))\n",
        "            train_acc = train_acc + [np.mean(train_accuracy)]\n",
        "            train_loss = train_loss + [np.mean(train_l)]\n",
        "            # calculate test accuracy after each training epoch\n",
        "            for j in range(batch_num_per_epoch):\n",
        "                ########################## prepare test data ########################\n",
        "                offset = (j * batch_size) % (test_y.shape[0] - batch_size)\n",
        "                test_batch_x = test_x[offset:(offset + batch_size), :, :, :]\n",
        "                test_batch_x = test_batch_x.reshape([len(test_batch_x) * num_timestep, n_channel, window_size, 1])\n",
        "                test_batch_y = test_y[offset:(offset + batch_size), :]\n",
        "\n",
        "                ########################## calculate test results ########################\n",
        "                test_a, test_c, prob_v, pred_v = session.run([accuracy, cost, y_prob, y_pred],\n",
        "                                                             feed_dict={X: test_batch_x, Y: test_batch_y,\n",
        "                                                                        keep_prob: 1.0, train_phase: False})\n",
        "\n",
        "                test_accuracy = np.append(test_accuracy, test_a)\n",
        "                test_l = np.append(test_l, test_c)\n",
        "                pred_test = np.append(pred_test, pred_v)\n",
        "                true_test.append(test_batch_y)\n",
        "                prob_test.append(prob_v)\n",
        "            if np.mean(test_accuracy) > best_acc:\n",
        "                best_acc = np.mean(test_accuracy)\n",
        "            true_test = np.array(true_test).reshape([-1, num_labels])\n",
        "            prob_test = np.array(prob_test).reshape([-1, num_labels])\n",
        "            auc_roc_test = multiclass_roc_auc_score(y_true=true_test, y_score=prob_test)\n",
        "            precision = precision_score(y_true=np.argmax(true_test, axis=1), y_pred=pred_test, average='macro')\n",
        "            recall = recall_score(y_true=np.argmax(true_test, axis=1), y_pred=pred_test, average='macro')\n",
        "            f1 = f1_score(y_true=np.argmax(true_test, axis=1), y_pred=pred_test, average='macro')\n",
        "\n",
        "            cost_arr.append(np.mean(test_l))\n",
        "            accuracy_arr.append(np.mean(test_accuracy))\n",
        "            f1_arr.append(f1)\n",
        "            auc_arr.append(auc_roc_test['macro'])\n",
        "            precision_arr.append(precision)\n",
        "            recall_arr.append(recall)\n",
        "            print(type(recall), recall)\n",
        "            \n",
        "\n",
        "            print(\"(\" + time.asctime(time.localtime(time.time())) + \") Epoch: \", epoch + 1, \"Test Cost: \",\n",
        "                  np.mean(test_l),\n",
        "                  \"Test Accuracy: \", np.mean(test_accuracy),\n",
        "                  \"Test f1: \", f1,\n",
        "                  \"Test AUC: \", auc_roc_test['macro'],\n",
        "                  \"Test Precision: \", precision,\n",
        "                  \"Test Recall: \", recall, \"\\n\")\n",
        "            \n",
        "    print(\"Accuracy:\")\n",
        "    print(np.mean(accuracy_arr), np.std(accuracy_arr))\n",
        "    print(\"Precision::\")\n",
        "    print(np.mean(precision_arr), np.std(precision_arr))\n",
        "    print(\"Recall Score:\")\n",
        "    print(np.mean(recall_arr), np.std(recall_arr))\n",
        "    print(\"F1 Score:\")\n",
        "    print(np.mean(f1_arr), np.std(f1_arr))\n",
        "    print(\"AUC:\")\n",
        "    print(np.mean(auc_arr), np.std(auc_arr))\n",
        "    print(\"accuracy_arr\")\n",
        "    print(accuracy_arr)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "subject id  1\n",
            "train_win_x shape:  (4032, 15, 400, 22)\n",
            "test_win_x shape:  (1152, 15, 400, 22)\n",
            "train_win_x shape:  (4032, 15, 22, 400)\n",
            "test_win_x shape:  (1152, 15, 22, 400)\n",
            "WARNING:tensorflow:From <ipython-input-3-5dd779426505>:68: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.BatchNormalization instead.  In particular, `tf.control_dependencies(tf.GraphKeys.UPDATE_OPS)` should not be used (consult the `tf.keras.layers.batch_normalization` documentation).\n",
            "WARNING:tensorflow:Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f05c9f8ddd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f05c9f8ddd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f05c9f8ddd8>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BatchNormalization.call of <tensorflow.python.layers.normalization.BatchNormalization object at 0x7f05c9f8ddd8>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "conv 1 shape:  [None, 1, 356, 40]\n",
            "pool 1 shape:  [None, 1, 29, 40]\n",
            "WARNING:tensorflow:From <ipython-input-5-0c728251711e>:182: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:\n",
            "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
            "For more information, please see:\n",
            "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
            "  * https://github.com/tensorflow/addons\n",
            "  * https://github.com/tensorflow/io (for I/O related ops)\n",
            "If you depend on functionality not listed there, please file an issue.\n",
            "\n",
            "WARNING:tensorflow:From <ipython-input-5-0c728251711e>:189: BasicLSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-5-0c728251711e>:192: MultiRNNCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.StackedRNNCells, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-5-0c728251711e>:197: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f05c9fc2b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f05c9fc2b38>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING: Entity <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f05c9fc2b38>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method MultiRNNCell.call of <tensorflow.python.ops.rnn_cell_impl.MultiRNNCell object at 0x7f05c9fc2b38>>: AttributeError: module 'gast' has no attribute 'Num'\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/rnn_cell_impl.py:738: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f05c9fc2278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f05c9fc2278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f05c9fc2278>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f05c9fc2278>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f05c4011fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f05c4011fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING: Entity <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f05c4011fd0>> could not be transformed and will be executed as-is. Please report this to the AutgoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: converting <bound method BasicLSTMCell.call of <tensorflow.python.ops.rnn_cell_impl.BasicLSTMCell object at 0x7f05c4011fd0>>: AssertionError: Bad argument number for Name: 3, expecting 4\n",
            "WARNING:tensorflow:From <ipython-input-5-0c728251711e>:216: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "\n",
            "Future major versions of TensorFlow will allow gradients to flow\n",
            "into the labels input on backprop by default.\n",
            "\n",
            "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
            "\n",
            "(Sat Apr 18 14:23:15 2020) Epoch:  1  Training Cost:  1.3863616031097714 Training Accuracy:  0.25012407479599746\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:32: DeprecationWarning: scipy.interp is deprecated and will be removed in SciPy 2.0.0, use numpy.interp instead\n",
            "/usr/local/lib/python3.6/dist-packages/sklearn/metrics/_classification.py:1272: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "<class 'numpy.float64'> 0.25\n",
            "(Sat Apr 18 14:23:22 2020) Epoch:  1 Test Cost:  1.3869143653448404 Test Accuracy:  0.24863524097145048 Test f1:  0.09956279809220987 Test AUC:  0.5158849723340883 Test Precision:  0.062158808933002484 Test Recall:  0.25 \n",
            "\n",
            "(Sat Apr 18 14:23:40 2020) Epoch:  2  Training Cost:  1.3853393642245688 Training Accuracy:  0.24987593583448353\n",
            "<class 'numpy.float64'> 0.25\n",
            "(Sat Apr 18 14:23:47 2020) Epoch:  2 Test Cost:  1.386281860673398 Test Accuracy:  0.24863524097145048 Test f1:  0.09956279809220987 Test AUC:  0.5309112618866809 Test Precision:  0.062158808933002484 Test Recall:  0.25 \n",
            "\n",
            "(Sat Apr 18 14:24:06 2020) Epoch:  3  Training Cost:  1.3826530946987143 Training Accuracy:  0.31265509348575293\n",
            "<class 'numpy.float64'> 0.31265516538775767\n",
            "(Sat Apr 18 14:24:13 2020) Epoch:  3 Test Cost:  1.3834610881639768 Test Accuracy:  0.3119106761759919 Test f1:  0.2578379156774118 Test AUC:  0.5587061216991707 Test Precision:  0.2577003851519569 Test Recall:  0.31265516538775767 \n",
            "\n",
            "Accuracy:\n",
            "0.2697270527062976 0.029828326210440816\n",
            "Precision::\n",
            "0.12733933433932063 0.09217918303221921\n",
            "Recall Score:\n",
            "0.27088505512925254 0.029535928214698737\n",
            "F1 Score:\n",
            "0.15232117062061054 0.07461160595839632\n",
            "AUC:\n",
            "0.5351674519733133 0.01773882885605915\n",
            "accuracy_arr\n",
            "[0.24863524097145048, 0.24863524097145048, 0.3119106761759919]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}